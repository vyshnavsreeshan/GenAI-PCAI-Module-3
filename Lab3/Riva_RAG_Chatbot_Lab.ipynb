{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NVIDIA Riva RAG Chatbot Lab**\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, you will build a **multilingual voice-enabled RAG chatbot** that combines NVIDIA Riva's speech AI capabilities with HPE AI Essentials Knowledge Base for intelligent, context-aware responses.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- How to use **NVIDIA Riva** for multilingual speech recognition and synthesis\n",
    "- How to integrate with **HPE AI Essentials Knowledge Base** for RAG-powered responses\n",
    "- How to build an interactive voice interface for enterprise AI applications\n",
    "\n",
    "### Technologies Used\n",
    "\n",
    "**NVIDIA Riva:**\n",
    "- **ASR (Automatic Speech Recognition)**: `nvidia/parakeet-1.1b-rnnt-multilingual-asr` - Converts speech to text with automatic language detection\n",
    "- **TTS (Text-to-Speech)**: `nvidia/magpie-tts-multilingual` - Converts text responses back to natural speech\n",
    "\n",
    "**HPE AI Essentials Knowledge Base:**\n",
    "- Retrieval-Augmented Generation (RAG) framework for context-aware responses\n",
    "- Vector database (Weaviate) for semantic search\n",
    "- LLM: `meta/llama3-8b-instruct` for response generation\n",
    "\n",
    "### Lab Flow\n",
    "\n",
    "1. **Setup**: Install dependencies and configure endpoints\n",
    "2. **Initialize Services**: Connect to Riva ASR and TTS services\n",
    "3. **Define Functions**: Create ASR, RAG, and TTS functions\n",
    "4. **Launch UI**: Start the interactive voice chatbot interface\n",
    "5. **Interact**: Ask questions using voice, file upload, or saved recordings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "The following diagram illustrates the complete architecture of our voice-enabled RAG chatbot system:\n",
    "\n",
    "![Riva RAG Architecture](./docs/Riva_RAG_Architecture.png)\n",
    "\n",
    "### Component Breakdown\n",
    "\n",
    "#### 1. **User Input** (Left - Teal Box)\n",
    "The system accepts audio input through three methods:\n",
    "- **ðŸŽ¤ Voice Input**: Real-time recording via browser microphone\n",
    "- **ðŸ“¤ File Upload**: Upload pre-recorded audio files (.wav, .mp3)\n",
    "- **ðŸ“ Saved Files**: Select from previously recorded/uploaded files in `./input/`\n",
    "\n",
    "#### 2. **NVIDIA Riva ASR** (Top Center - Green Box)\n",
    "Converts audio to text using the **Parakeet 1.1B Multilingual ASR** model:\n",
    "- âœ“ **Auto Language Detection**: Automatically identifies the spoken language\n",
    "- âœ“ **Multi-language Support**: English, Spanish, French, German, Chinese, and more\n",
    "- âœ“ **Automatic Punctuation**: Adds proper punctuation for readability\n",
    "\n",
    "**Output**: Transcribed text sent to the RAG system\n",
    "\n",
    "#### 3. **HPE AI Essentials Knowledge Base (RAG)** (Right - Teal Box)\n",
    "The RAG pipeline processes queries through multiple stages:\n",
    "\n",
    "**a. Vector Database (Weaviate)**\n",
    "- Stores enterprise documents as high-dimensional vector embeddings\n",
    "- Enables fast semantic similarity search\n",
    "\n",
    "**b. Embedding Model (NVIDIA Retrieval QA E5 v5)**\n",
    "- Converts text queries into numerical vectors\n",
    "- Captures semantic meaning for accurate retrieval\n",
    "\n",
    "**c. Semantic Search & Context Retrieval**\n",
    "- Searches the vector database for relevant context\n",
    "- Retrieves the most pertinent information for the query\n",
    "\n",
    "**d. LLM Generation (Meta Llama3-8B-Instruct)**\n",
    "- Generates responses based on retrieved context\n",
    "- Follows system instructions to avoid hallucinations\n",
    "- Produces responses in the selected output language\n",
    "\n",
    "**Output**: Context-aware response text\n",
    "\n",
    "#### 4. **Language Selection** (Left Center - Gray Box)\n",
    "User-selectable output language:\n",
    "- ðŸŒ English, Spanish, French, German, Chinese\n",
    "- Sends **Output Language Code** to both RAG and TTS (dashed lines)\n",
    "- RAG generates text in the selected language\n",
    "- TTS synthesizes speech with appropriate accent\n",
    "\n",
    "#### 5. **NVIDIA Riva TTS** (Bottom Center - Green Box)\n",
    "Converts text responses to natural speech using **Magpie Multilingual TTS**:\n",
    "- âœ“ **Natural Voices**: Human-like speech synthesis\n",
    "- âœ“ **Multi-language**: Supports multiple languages with native accents\n",
    "- âœ“ **44.1kHz Audio**: High-quality audio output\n",
    "\n",
    "**Output**: Audio response file\n",
    "\n",
    "#### 6. **User Output** (Bottom Left - Teal Box)\n",
    "The system presents results in multiple formats:\n",
    "- **ðŸ”Š Audio Playback**: Auto-plays the synthesized speech\n",
    "- **ðŸ’¾ Saved to ./outputs/**: Stores TTS audio files for later review\n",
    "- **ðŸ“ Text Display**: Shows the transcribed query and RAG response\n",
    "\n",
    "### Data Flow\n",
    "\n",
    "```\n",
    "1. User speaks or uploads audio â†’ Audio File\n",
    "2. NVIDIA Riva ASR processes â†’ Transcribed Text\n",
    "3. RAG system retrieves context â†’ Context-Aware Response\n",
    "4. NVIDIA Riva TTS synthesizes â†’ Audio Response\n",
    "5. User receives audio + text output\n",
    "```\n",
    "\n",
    "### Key Features\n",
    "\n",
    "âœ… **Multilingual Input**: Speak in any supported language (auto-detected)  \n",
    "âœ… **Multilingual Output**: Receive responses in your preferred language  \n",
    "âœ… **Context-Aware**: RAG retrieves relevant information from your knowledge base  \n",
    "âœ… **Voice-Enabled**: Complete hands-free interaction  \n",
    "âœ… **Persistent Storage**: All audio files saved for review  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Install Dependencies\n",
    "\n",
    "Install the NVIDIA Riva client library and other required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nvidia-riva-client requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Configuration\n",
    "\n",
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wave\n",
    "import json\n",
    "import requests\n",
    "import riva.client\n",
    "import IPython.display as ipd\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Endpoints and Models\n",
    "\n",
    "**Important:** Replace `YOUR_AUTH_TOKEN_HERE` and `APP_NAME_HERE` with your actual Knowledge Base authentication token and metadata name.\n",
    "\n",
    "You can retrieve your token and metadata name from the HPE AI Essentials Knowledge Base settings page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Riva Configuration\n",
    "RIVA_URI = \"10.179.253.43:32222\"\n",
    "\n",
    "# RAG Configuration\n",
    "RAG_ENDPOINT = \"https://rag-coordinator.pcai2.genai2.hou\"\n",
    "RAG_API_PATH = \"/v1/chat/completions\"\n",
    "AUTH_TOKEN = \"YOUR_AUTH_TOKEN_HER\"  # Replace with your actual token\n",
    "APP_NAME = \"APP_NAME_HERE\"  # Replace with your actual APP Name \n",
    "ENABLE_CITATIONS = \"false\"\n",
    "\n",
    "# Models\n",
    "ASR_MODEL = \"nvidia/parakeet-1.1b-rnnt-multilingual-asr\"\n",
    "TTS_MODEL = \"nvidia/magpie-tts-multilingual\"\n",
    "RAG_MODEL = \"meta/llama3-8b-instruct\"\n",
    "\n",
    "print(f\"Riva URI: {RIVA_URI}\")\n",
    "print(f\"RAG Endpoint: {RAG_ENDPOINT}{RAG_API_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define System Context\n",
    "\n",
    "This system prompt instructs the RAG model to provide responses based strictly on retrieved context from the Knowledge Base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Context for RAG\n",
    "SYSTEM_CONTEXT = \"\"\"- You are an AI assistant who analyzes the input query and provides responses strictly based on the retrieved context and previous chat conversations.\n",
    "- If no relevant context or prior chat conversations is available, do not respond and instead state: \"I don't have enough information to answer the question.\"\n",
    "- You must not rely on general knowledge or any data from your training. Use only the specific details from the given context and prior chat conversations.\n",
    "- If no relevant context is retrieved, do not attempt to generate a response.\n",
    "- Your answers must remain strictly within the boundaries of the available context and previous chat conversations.\n",
    "- If the user sends a greeting or a polite remark, reply briefly in a friendly manner\n",
    "- Do not include greetings in responses to non-greeting queries.\n",
    "- Don't assume or hallucinate when responding\n",
    "- Do not include the phrase 'Based on the provided context and previous chat conversation' in the responses.\n",
    "\n",
    "Context:\n",
    "{context} \n",
    "\n",
    "Chat_Conversations:\n",
    "{chat_history}\n",
    "\n",
    "Query:\n",
    "{query}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Initialize Riva Services\n",
    "\n",
    "Connect to the NVIDIA Riva server and initialize ASR and TTS services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    auth = riva.client.Auth(uri=RIVA_URI)\n",
    "    asr_service = riva.client.ASRService(auth)\n",
    "    tts_service = riva.client.SpeechSynthesisService(auth)\n",
    "    print(\"âœ“ Riva services initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Error initializing Riva services: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Define ASR Function\n",
    "\n",
    "This function transcribes audio files using the **Parakeet multilingual ASR model** with automatic language detection.\n",
    "\n",
    "**Key Features:**\n",
    "- Supports multiple languages (English, Spanish, French, German, Chinese, and more)\n",
    "- Automatically detects the spoken language\n",
    "- Enables automatic punctuation for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_file):\n",
    "    \"\"\"Transcribe audio using Riva ASR with automatic language detection.\"\"\"\n",
    "    try:\n",
    "        with open(audio_file, 'rb') as fh:\n",
    "            data = fh.read()\n",
    "\n",
    "        config = riva.client.RecognitionConfig(\n",
    "            encoding=riva.client.AudioEncoding.LINEAR_PCM,\n",
    "            max_alternatives=1,\n",
    "            enable_automatic_punctuation=True,\n",
    "            verbatim_transcripts=False,\n",
    "            language_code=\"multi\"  # Auto-detect language\n",
    "        )\n",
    "        \n",
    "        # Helper to set sample_rate_hz and audio_channel_count from the WAV file header\n",
    "        riva.client.add_audio_file_specs_to_config(config, audio_file)\n",
    "        \n",
    "        response = asr_service.offline_recognize(data, config)\n",
    "        if len(response.results) > 0:\n",
    "            transcript = response.results[0].alternatives[0].transcript\n",
    "            return transcript\n",
    "        else:\n",
    "            return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"ASR Error: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Define RAG Query Function\n",
    "\n",
    "This function sends the transcribed text to the **HPE AI Essentials Knowledge Base** RAG endpoint.\n",
    "\n",
    "**How it works:**\n",
    "1. Receives the user's transcribed query\n",
    "2. Adds language instruction to the system context\n",
    "3. Sends request to the RAG coordinator\n",
    "4. Retrieves context from the vector database\n",
    "5. Generates a response using the LLM\n",
    "\n",
    "**Note:** The Knowledge Base handles context retrieval, embeddings, and chat history automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(text, output_language=\"English\"):\n",
    "    \"\"\"Query RAG endpoint with language-specific instruction.\"\"\"\n",
    "    url = f\"{RAG_ENDPOINT}{RAG_API_PATH}\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {AUTH_TOKEN}\",\n",
    "        \"X-SA-NAME\": APP_NAME,\n",
    "        \"X-ENABLE-CITATIONS\": ENABLE_CITATIONS\n",
    "    }\n",
    "    \n",
    "    # Add language instruction to the system context\n",
    "    language_instruction = f\"\\n\\nIMPORTANT: Provide your response in {output_language} language.\"\n",
    "    system_context_with_language = SYSTEM_CONTEXT + language_instruction\n",
    "    \n",
    "    # Payload for Chat Completion\n",
    "    payload = {\n",
    "        \"model\": RAG_MODEL,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_context_with_language},\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ],\n",
    "        \"max_tokens\": 2048,\n",
    "        \"temperature\": 0,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 1,\n",
    "        \"top_p\": 0.5\n",
    "    }\n",
    "    \n",
    "    print(f\"Querying Knowledge Base... (Requesting {output_language} response)\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=payload, verify=False) \n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        \n",
    "        # Handle Chat Completion Response\n",
    "        if \"choices\" in result and len(result[\"choices\"]) > 0:\n",
    "            answer = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "            return answer\n",
    "        else:\n",
    "            return f\"Unexpected response format: {str(result)}\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"RAG Error: {e}\")\n",
    "        return \"I'm sorry, I couldn't connect to the knowledge base.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Define TTS Function\n",
    "\n",
    "This function converts text responses to speech using the **Magpie multilingual TTS model**.\n",
    "\n",
    "**Key Features:**\n",
    "- Supports multiple languages with natural-sounding voices\n",
    "- Saves audio files to `./outputs/` directory\n",
    "- Auto-plays the generated audio in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter for unique filenames\n",
    "tts_counter = 0\n",
    "OUTPUT_DIR = \"./outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def speak_text(text, language_code=\"en-US\"):\n",
    "    \"\"\"Synthesize speech in the specified language.\"\"\"\n",
    "    global tts_counter\n",
    "    try:\n",
    "        print(f\"Synthesizing speech: {text[:50]}...\")\n",
    "        sample_rate_hz = 44100\n",
    "        nchannels = 1\n",
    "        sampwidth = 2\n",
    "        \n",
    "        response = tts_service.synthesize(\n",
    "            text,\n",
    "            language_code=language_code,\n",
    "            sample_rate_hz=sample_rate_hz\n",
    "        )\n",
    "        \n",
    "        # Save to output folder\n",
    "        tts_counter += 1\n",
    "        output_file = os.path.join(OUTPUT_DIR, f\"response_{tts_counter}.wav\")\n",
    "        \n",
    "        with wave.open(output_file, 'wb') as out_f:\n",
    "            out_f.setnchannels(nchannels)\n",
    "            out_f.setsampwidth(sampwidth)\n",
    "            out_f.setframerate(sample_rate_hz)\n",
    "            out_f.writeframesraw(response.audio)\n",
    "        \n",
    "        print(f\"âœ“ Audio saved to: {output_file}\")\n",
    "        display(ipd.Audio(output_file, autoplay=True))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"TTS Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Launch Interactive Chatbot UI\n",
    "\n",
    "The chatbot UI provides three ways to interact:\n",
    "\n",
    "### Input Options:\n",
    "1. **Record Audio**: Click \"Start Recording\" to record your question, then \"Stop Recording\" when done\n",
    "2. **Upload File**: Upload a pre-recorded audio file (.wav or .mp3)\n",
    "3. **Select Saved File**: Choose from previously recorded or uploaded files in the `./input/` directory\n",
    "\n",
    "### Output Language:\n",
    "Select your preferred output language from the dropdown:\n",
    "- English (US)\n",
    "- Spanish (US)\n",
    "- French\n",
    "- German\n",
    "- Chinese\n",
    "\n",
    "### Complete Workflow:\n",
    "```\n",
    "Voice Input â†’ ASR (auto-detect language) â†’ RAG Query â†’ LLM Response â†’ TTS (selected language)\n",
    "```\n",
    "\n",
    "**Example:** Speak in English, get response generated and spoken in Spanish!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatbot_ui import create_chatbot_ui\n",
    "\n",
    "# Launch the chatbot UI\n",
    "create_chatbot_ui(transcribe_audio, query_rag, speak_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Congratulations! You've built a multilingual voice-enabled RAG chatbot that combines:\n",
    "\n",
    "âœ… **NVIDIA Riva ASR** - Multilingual speech recognition with auto-detection  \n",
    "âœ… **HPE AI Essentials Knowledge Base** - Context-aware RAG responses from your enterprise data  \n",
    "âœ… **NVIDIA Riva TTS** - Natural multilingual speech synthesis  \n",
    "\n",
    "### Key Takeaways:\n",
    "- Riva's Parakeet model automatically detects the spoken language\n",
    "- Knowledge Base retrieves relevant context from your vector database\n",
    "- Responses can be generated and spoken in any supported language\n",
    "- All audio files are saved for future reference\n",
    "\n",
    "### Next Steps:\n",
    "- Explore the **Knowledge Base Lab** to learn how to build and manage your own RAG solutions\n",
    "- Experiment with different languages and queries\n",
    "- Review saved audio files in `./input/` and `./outputs/` directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

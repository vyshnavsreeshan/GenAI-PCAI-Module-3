{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "3e9c587a",
            "metadata": {},
            "source": [
                "# **NVIDIA Riva RAG Lab: End-to-End Voice Chatbot**\n",
                "\n",
                "## **1. Introduction**\n",
                "Welcome to the **Riva RAG Ingestion Lab**. This notebook demonstrates how to build a complete **Voice-Enabled Retrieval Augmented Generation (RAG)** system from scratch. Unlike previous versions, this lab allows you to **ingest your own PDF documents**, create a vector index, and then query it using your voice.\n",
                "\n",
                "## **2. Objective**\n",
                "By the end of this lab, you will have built:\n",
                "1.  A **Knowledge Base** created from raw PDF files.\n",
                "2.  A **RAG Pipeline** that retrieves relevant context to answer questions.\n",
                "3.  A **Voice Interface** that listens to your speech and responds with synthesized audio.\n",
                "\n",
                "## **3. Technologies Used**\n",
                "- **NVIDIA Riva**: For high-performance Automatic Speech Recognition (ASR) and Text-to-Speech (TTS).\n",
                "- **LangChain**: For orchestrating the RAG pipeline.\n",
                "- **FAISS**: For efficient vector storage and similarity search.\n",
                "- **NVIDIA Embeddings (NV-EmbedQA-E5)**: For generating semantic vectors.\n",
                "- **Meta Llama 3 (8B)**: For generating intelligent responses.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e457a52d",
            "metadata": {},
            "source": [
                "## **4. Architecture Overview**\n",
                "\n",
                "![Riva RAG Architecture](docs/Riva_RAG_Architecture.png)\n",
                "\n",
                "### **System Flow Explanation**\n",
                "The diagram above illustrates the complete **Voice-Enabled RAG Pipeline**:\n",
                "\n",
                "1.  **Ingestion (Left Side)**:\n",
                "    - **PDF Documents**: Raw files are loaded from the `./pdf` directory.\n",
                "    - **Preprocessing**: Text is extracted and split into chunks.\n",
                "    - **Embedding**: The NVIDIA Embedding model converts text into vectors (using `input_type=\"passage\"`).\n",
                "    - **Vector Store**: A FAISS index stores these vectors locally.\n",
                "\n",
                "2.  **Voice Interaction (Right Side)**:\n",
                "    - **User Voice**: Your spoken question is captured.\n",
                "    - **ASR (Automatic Speech Recognition)**: NVIDIA Riva translates audio to text.\n",
                "    - **RAG Retrieval**: The system queries the FAISS index (using `input_type=\"query\"`) to find relevant context.\n",
                "    - **LLM Generation**: The Meta Llama 3 model generates a concise answer based on the retrieved context.\n",
                "    - **TTS (Text-to-Speech)**: NVIDIA Riva converts the text answer back into natural-sounding speech.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "94cca6d7",
            "metadata": {},
            "source": [
                "### Scenario: InnovateSphere Corporation\n",
                "\n",
                "We'll work with documents from InnovateSphere, a fictional company with three departments:\n",
                "- **HR**: Employee policies, leave entitlements, code of conduct\n",
                "- **Marketing**: Brand guidelines, campaigns, social media policies  \n",
                "- **Sales**: Product catalogs, sales reports, team directories\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5c3340c6",
            "metadata": {},
            "source": [
                "## **Step 1: Environment Setup**\n",
                "\n",
                "### **Dependency Resolution**\n",
                "The cell below performs a \"clean slate\" installation:\n",
                "- **Uninstalls** conflicting versions of LangChain and Pydantic.\n",
                "- **Installs** a pinned, compatible set of libraries (LangChain 0.2.x, Pydantic v1) to ensure stability.\n",
                "\n",
                "> **Note:** You may need to restart the kernel after running this cell."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "32127607",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dependency Resolution (Clean Slate)\n",
                "# 1. Uninstall specific conflicting packages first\n",
                "%pip uninstall -y langchain langchain-core langchain-community langchain-text-splitters langchain-openai pydantic -q\n",
                "\n",
                "# 2. Install pinned compatible versions (LangChain 0.2.x, Pydantic v1)\n",
                "%pip install --upgrade \"pydantic<2.0.0\" \"langchain==0.2.14\" \"langchain-community==0.2.12\" \"langchain-core==0.2.33\" \"langchain-text-splitters==0.2.2\" \"langchain-openai==0.1.22\" \"nvidia-riva-client\" \"requests\" \"faiss-cpu\" \"httpx\" \"pypdf\" -q\n",
                "\n",
                "# 3. Verify versions\n",
                "%pip list | grep -E \"langchain|pydantic\""
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e1606517",
            "metadata": {},
            "source": [
                "### **Import Libraries**\n",
                "Import necessary Python modules for File I/O, Networking, Riva Client, and LangChain components."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5f049ba1",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import wave\n",
                "import json\n",
                "import requests\n",
                "import riva.client\n",
                "import IPython.display as ipd\n",
                "from IPython.display import display\n",
                "import httpx\n",
                "\n",
                "# LangChain Imports\n",
                "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
                "from langchain_community.vectorstores import FAISS\n",
                "from langchain.chains import create_retrieval_chain\n",
                "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
                "from langchain_core.prompts import ChatPromptTemplate\n",
                "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
                "from langchain_text_splitters import RecursiveCharacterTextSplitter"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6e3b0873",
            "metadata": {},
            "source": [
                "### **Log Suppression**\n",
                "To keep the notebook output clean, we suppress verbose logging from the HTTP client and FAISS."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7ab0a72a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Suppress Logging\n",
                "import logging\n",
                "import os\n",
                "\n",
                "# Suppress HTTPX logs (POST requests)\n",
                "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
                "logging.getLogger(\"httpcore\").setLevel(logging.WARNING)\n",
                "\n",
                "# Suppress FAISS logs (via environment variable - requires restart if already loaded)\n",
                "\n",
                "print(\"Verbose logging suppressed.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4e1d920c",
            "metadata": {},
            "source": [
                "## **Step 2: Configuration**\n",
                "\n",
                "Here we define the API endpoints and Authentication Tokens.\n",
                "\n",
                "> **IMPORTANT:** You must replace `PLACEHOLDER_EMBEDDING_ENDPOINT`, `PLACEHOLDER_LLM_ENDPOINT`with your actual endpoints and `PLACEHOLDER_EMBEDDING_TOKEN`, `PLACEHOLDER_LLM_TOKEN` with your actual keys."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bf12364d",
            "metadata": {},
            "source": [
                "### **How to Generate API Tokens**\n",
                "\n",
                "To run this lab, you need endpoints and tokens for the **NVIDIA Embedding** and **Llama 3** models. Follow these steps:\n",
                "\n",
                "**1. Access Gen AI Studio**\n",
                "Open the Generative AI Studio from your dashboard.\n",
                "![Access Gen AI](docs/accessing_Gen%20AI.png)\n",
                "\n",
                "**2. Navigate to Model Endpoints**\n",
                "Click on the \"Model Endpoints\" option in the side menu.\n",
                "![Model Endpoints](docs/Model_endpoints_option.png)\n",
                "\n",
                "**3. Select a Model**\n",
                "Locate and select the model you need (e.g., `meta/llama3-8b-instruct` or `nvidia/nv-embedqa-e5-v5`).\n",
                "![Select Model](docs/accessing%20the%20model%20endpoint.png)\n",
                "\n",
                "**4. Copy the Endpoint URL**\n",
                "Copy the \"Endpoint URL\" shown on the screen. Paste this into the `EMBED_ENDPOINT` or `LLM_ENDPOINT` variable below.\n",
                "![Copy Endpoint](docs/copy_endpoint.png)\n",
                "\n",
                "**5. Generate Token**\n",
                "Click the **\"Generate Token\"** button.\n",
                "![Generate Button](docs/Generate_token_button.png)\n",
                "\n",
                "**6. Copy the API Key**\n",
                "A token will be generated. Copy this key immediately.\n",
                "![Copy Key](docs/copy_key.png)\n",
                "\n",
                "> **Repeat** this process for both the **Embedding Model** and the **LLM**.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2f7dc44f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "\n",
                "# API Endpoints\n",
                "EMBED_ENDPOINT = \"PLACEHOLDER_EMBEDDING_ENDPOINT\"\n",
                "LLM_ENDPOINT = \"PLACEHOLDER_LLM_ENDPOINT\"\n",
                "\n",
                "# Authentication Tokens (Update these!)\n",
                "EMBEDDING_AUTH_TOKEN = \"PLACEHOLDER_EMBEDDING_TOKEN\"\n",
                "LLM_AUTH_TOKEN = \"PLACEHOLDER_LLM_TOKEN\"\n",
                "\n",
                "# Riva URI\n",
                "RIVA_URI = \"10.179.253.43:32222\"\n",
                "\n",
                "# Directories\n",
                "OUTPUT_DIR = \"./outputs\"\n",
                "PDF_DIR = \"./pdf\"\n",
                "EMBEDDING_DIR = \"./embeddings\"\n",
                "\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "os.makedirs(PDF_DIR, exist_ok=True)\n",
                "os.makedirs(EMBEDDING_DIR, exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e64cb779",
            "metadata": {},
            "outputs": [],
            "source": [
                "# HTTP Client\n",
                "http_client = httpx.Client(verify=False)\n",
                "\n",
                "# 1. Ingestion Embeddings (Passage)\n",
                "ingestion_embeddings = OpenAIEmbeddings(\n",
                "    base_url=f\"{EMBED_ENDPOINT}/v1\",\n",
                "    api_key=EMBEDDING_AUTH_TOKEN,\n",
                "    model=\"nvidia/nv-embedqa-e5-v5\",\n",
                "    check_embedding_ctx_length=False,\n",
                "    http_client=http_client,\n",
                "    model_kwargs={\"extra_body\": {\"input_type\": \"passage\"}}\n",
                ")\n",
                "\n",
                "# 2. Retrieval Embeddings (Query)\n",
                "retrieval_embeddings = OpenAIEmbeddings(\n",
                "    base_url=f\"{EMBED_ENDPOINT}/v1\",\n",
                "    api_key=EMBEDDING_AUTH_TOKEN,\n",
                "    model=\"nvidia/nv-embedqa-e5-v5\",\n",
                "    check_embedding_ctx_length=False,\n",
                "    http_client=http_client,\n",
                "    model_kwargs={\"extra_body\": {\"input_type\": \"query\"}}\n",
                ")\n",
                "\n",
                "# 3. LLM\n",
                "llm = ChatOpenAI(\n",
                "    base_url=f\"{LLM_ENDPOINT}/v1\",\n",
                "    api_key=LLM_AUTH_TOKEN,\n",
                "    model=\"meta/llama3-8b-instruct\",\n",
                "    temperature=0.1,\n",
                "    max_tokens=1024,\n",
                "    http_client=http_client\n",
                ")\n",
                "\n",
                "print(\"Components initialized successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3dd961a1",
            "metadata": {},
            "source": [
                "## **Step 4: Data Ingestion (Knowledge Base Creation)**\n",
                "\n",
                "In this step, we build the \"Brain\" of our chatbot:\n",
                "1.  **Load**: Read all PDF files from the `./pdf` directory.\n",
                "2.  **Split**: Break the text into smaller chunks (800 characters) to fit into the models' context window.\n",
                "3.  **Embed & Index**: Convert text chunks into vectors and save them locally using FAISS.\n",
                "\n",
                "> **Action**: Ensure you have uploaded at least one PDF to the `pdf` folder!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8f4fbbef",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Loading PDFs from {PDF_DIR}...\")\n",
                "loader = PyPDFDirectoryLoader(PDF_DIR)\n",
                "data = loader.load()\n",
                "\n",
                "if not data:\n",
                "    print(\"WARNING: No PDF documents found! Please upload files to the ./pdf folder.\")\n",
                "else:\n",
                "    print(f\"Loaded {len(data)} pages.\")\n",
                "    \n",
                "    # Split Text\n",
                "    text_splitter = RecursiveCharacterTextSplitter(\n",
                "        chunk_size=800,\n",
                "        chunk_overlap=60,\n",
                "    )\n",
                "    documents = text_splitter.split_documents(data)\n",
                "    print(f\"Created {len(documents)} text chunks.\")\n",
                "    \n",
                "    # Create Index (using ingestion_embeddings)\n",
                "    print(\"Creating FAISS index (this may take a moment)...\")\n",
                "    docsearch = FAISS.from_documents(documents, embedding=ingestion_embeddings)\n",
                "    \n",
                "    # Save Index\n",
                "    docsearch.save_local(folder_path=EMBEDDING_DIR)\n",
                "    print(f\"Index saved to {EMBEDDING_DIR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "73ca025b",
            "metadata": {},
            "source": [
                "## **Step 5: RAG Pipeline Setup**\n",
                "\n",
                "Now that we have our index, we set up the retrieval chain:\n",
                "1.  **Load Index**: Read the FAISS index from disk using the `retrieval_embeddings`.\n",
                "2.  **Prompt**: Define how the LLM should behave. We enforce concise answers for better Voice interaction.\n",
                "3.  **Chain**: Combine the Retriever + Prompt + LLM into a single callable object."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8710efe0",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Index (using retrieval_embeddings)\n",
                "try:\n",
                "    vector_store = FAISS.load_local(\n",
                "        EMBEDDING_DIR, \n",
                "        retrieval_embeddings, \n",
                "        allow_dangerous_deserialization=True\n",
                "    )\n",
                "    retriever = vector_store.as_retriever()\n",
                "    print(\"FAISS index loaded for retrieval.\")\n",
                "\n",
                "    # Prompt Template\n",
                "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
                "    You are a voice assistant. Answer strictly based on the provided context.\n",
                "    \n",
                "    IMPORTANT GUIDELINES:\n",
                "    1. Be extremely concise. Keep your answer under 2 sentences.\n",
                "    2. Do not use bullet points or special formatting.\n",
                "    3. If the answer is not in the context, say \"I don't know.\"\n",
                "    \n",
                "    <context>\n",
                "    {context}\n",
                "    </context>\n",
                "    \n",
                "    Query: {input}\n",
                "    \n",
                "    Answer in {language}:\n",
                "    \"\"\")\n",
                "\n",
                "    # Create Chain\n",
                "    document_chain = create_stuff_documents_chain(llm, prompt)\n",
                "    retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
                "    print(\"RAG Chain ready.\")\n",
                "    \n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"Error loading index: {e}\")\n",
                "    print(\"Did you run the ingestion step above?\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2e108783",
            "metadata": {},
            "source": [
                "## **Step 6: Initialize NVIDIA Riva**\n",
                "\n",
                "We set up the **Riva Client** to access the high-performance Speech AI services running on the cluster.\n",
                "- **ASRService**: For transcribing your microphone input.\n",
                "- **SpeechSynthesisService**: For speaking the AI's response."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7ab53f82",
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    auth = riva.client.Auth(uri=RIVA_URI)\n",
                "    asr_service = riva.client.ASRService(auth)\n",
                "    tts_service = riva.client.SpeechSynthesisService(auth)\n",
                "    print(\"✓ Riva services initialized.\")\n",
                "except Exception as e:\n",
                "    print(f\"✗ Error initializing Riva: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e628446e",
            "metadata": {},
            "source": [
                "## **Step 7: Define ASR Function**\n",
                "\n",
                "This function transcribes audio files using the **Parakeet multilingual ASR model** with automatic language detection.\n",
                "\n",
                "**Key Features:**\n",
                "- Supports multiple languages (English, Spanish, French, German, Chinese, and more)\n",
                "- Automatically detects the spoken language\n",
                "- Enables automatic punctuation for better readability"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b334d5eb",
            "metadata": {},
            "outputs": [],
            "source": [
                "def transcribe_audio(audio_file):\n",
                "    try:\n",
                "        with open(audio_file, 'rb') as fh:\n",
                "            data = fh.read()\n",
                "        config = riva.client.RecognitionConfig(\n",
                "            encoding=riva.client.AudioEncoding.LINEAR_PCM,\n",
                "            max_alternatives=1,\n",
                "            enable_automatic_punctuation=True,\n",
                "            verbatim_transcripts=False,\n",
                "            language_code=\"multi\"\n",
                "        )\n",
                "        riva.client.add_audio_file_specs_to_config(config, audio_file)\n",
                "        response = asr_service.offline_recognize(data, config)\n",
                "        if len(response.results) > 0:\n",
                "            return response.results[0].alternatives[0].transcript\n",
                "        return \"\"\n",
                "    except Exception as e:\n",
                "        print(f\"ASR Error: {e}\")\n",
                "        return \"\""
            ]
        },
        {
            "cell_type": "markdown",
            "id": "90eade67",
            "metadata": {},
            "source": [
                "## **Step 7: Define RAG Query Function**\n",
                "\n",
                "This function sends the transcribed text to the **retrieval_chain**.\n",
                "\n",
                "**How it works:**\n",
                "1. Receives the user's transcribed query\n",
                "2. Adds language instruction to the system context\n",
                "3. Invoke the retrieval_chain.\n",
                "4. Retrieves context from the vector database\n",
                "5. Generates a response using the LLM"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5d3a3def",
            "metadata": {},
            "outputs": [],
            "source": [
                "def query_rag(text, output_language=\"English\"):\n",
                "    try:\n",
                "        response = retrieval_chain.invoke({\"input\": text, \"language\": output_language})\n",
                "        return response[\"answer\"]\n",
                "    except Exception as e:\n",
                "        print(f\"RAG Error: {e}\")\n",
                "        return \"Error retrieving answer.\"\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2c12b4ce",
            "metadata": {},
            "source": [
                "## Step 8. Define TTS Function\n",
                "\n",
                "This function converts text responses to speech using the **Magpie multilingual TTS model**.\n",
                "\n",
                "**Key Features:**\n",
                "- Supports multiple languages with natural-sounding voices\n",
                "- Saves audio files to `./outputs/` directory\n",
                "- Auto-plays the generated audio in the notebook"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "888a517e",
            "metadata": {},
            "outputs": [],
            "source": [
                "tts_counter = 0\n",
                "def speak_text(text, language_code=\"en-US\"):\n",
                "    global tts_counter\n",
                "    try:\n",
                "        print(f\"Synthesizing: {text[:50]}...\")\n",
                "        response = tts_service.synthesize(\n",
                "            text, language_code=language_code, sample_rate_hz=44100\n",
                "        )\n",
                "        tts_counter += 1\n",
                "        output_file = os.path.join(OUTPUT_DIR, f\"response_{tts_counter}.wav\")\n",
                "        with wave.open(output_file, 'wb') as out_f:\n",
                "             out_f.setnchannels(1)\n",
                "             out_f.setsampwidth(2)\n",
                "             out_f.setframerate(44100)\n",
                "             out_f.writeframesraw(response.audio)\n",
                "        display(ipd.Audio(output_file, autoplay=True))\n",
                "    except Exception as e:\n",
                "        print(f\"TTS Error: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4dca21dc",
            "metadata": {},
            "source": [
                "## **Step 9: Launch Chatbot**\n",
                "\n",
                "Finally, we launch the interactive UI.\n",
                "- **Record**: Click to speak your question.\n",
                "- **Process**: The system transcribes audio -> Queries RAG -> Generates Answer -> Synthesizes Speech.\n",
                "- **Listen**: The answer will be played back automatically."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from chatbot_ui import create_chatbot_ui\n",
                "create_chatbot_ui(transcribe_audio, query_rag, speak_text)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}

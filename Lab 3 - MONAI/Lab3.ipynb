{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6286986e",
   "metadata": {},
   "source": [
    "Copyright (c) MONAI Consortium\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "    \n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "\n",
    "# MRI Brain image generation\n",
    "\n",
    "* This tutorial illustrates a generative model for creating 3D brain MRI from Gaussian noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301dab0b",
   "metadata": {},
   "source": [
    "## Setup environment\n",
    "\n",
    "* installing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b6f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q \"monai-weekly[nibabel, skimage, scipy, pillow, tensorboard, gdown, ignite, torchvision, itk, tqdm, lmdb, psutil, cucim, openslide, pandas, einops, transformers, mlflow, clearml, matplotlib, tensorboardX, tifffile, imagecodecs, pyyaml, fire, jsonschema, ninja, pynrrd, pydicom, h5py, nni, optuna, onnx, onnxruntime, zarr, lpips, pynvml, huggingface_hub]\"==\"1.5.dev2504\"\n",
    "%pip install -q nibabel\n",
    "%pip install -q ipywidgets\n",
    "%pip install -q opencv-python-headless==\"4.11.0.86\"\n",
    "%pip install -q matplotlib\n",
    "%pip install -q numpy==\"2.3.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc01d24",
   "metadata": {},
   "source": [
    "## Setup imports\n",
    "* importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e2019e-1556-41a6-95e8-5d1a65f8b3a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from monai.transforms import AsDiscrete\n",
    "from monai.config import print_config\n",
    "from monai.transforms import LoadImage, Orientation\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c8faae",
   "metadata": {},
   "source": [
    "* plotting helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "939b8c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_label_center_loc(x):\n",
    "    \"\"\"\n",
    "    Find the center location of non-zero elements in a binary mask.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Binary mask tensor. Expected shape: [H, W, D] or [C, H, W, D].\n",
    "\n",
    "    Returns:\n",
    "        list: Center locations for each dimension. Each element is either\n",
    "              the middle index of non-zero locations or None if no non-zero elements exist.\n",
    "    \"\"\"\n",
    "    label_loc = torch.where(x != 0)\n",
    "    center_loc = []\n",
    "    for loc in label_loc:\n",
    "        unique_loc = torch.unique(loc)\n",
    "        if len(unique_loc) == 0:\n",
    "            center_loc.append(None)\n",
    "        else:\n",
    "            center_loc.append(unique_loc[len(unique_loc) // 2])\n",
    "\n",
    "    return center_loc\n",
    "\n",
    "\n",
    "def normalize_label_to_uint8(colorize, label, n_label):\n",
    "    \"\"\"\n",
    "    Normalize and colorize a label tensor to a uint8 image.\n",
    "\n",
    "    Args:\n",
    "        colorize (torch.Tensor): Weight tensor for colorization. Expected shape: [3, n_label, 1, 1].\n",
    "        label (torch.Tensor): Input label tensor. Expected shape: [1, H, W].\n",
    "        n_label (int): Number of unique labels.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Normalized and colorized image as uint8 numpy array. Shape: [H, W, 3].\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        post_label = AsDiscrete(to_onehot=n_label)\n",
    "        label = post_label(label).permute(1, 0, 2, 3)\n",
    "        label = F.conv2d(label, weight=colorize)\n",
    "        label = torch.clip(label, 0, 1).squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    draw_img = (label * 255).astype(np.uint8)\n",
    "\n",
    "    return draw_img\n",
    "\n",
    "\n",
    "def visualize_one_slice_in_3d(image, axis: int = 2, center=None, mask_bool=True, n_label=105, colorize=None):\n",
    "    \"\"\"\n",
    "    Extract and visualize a 2D slice from a 3D image or label tensor.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): Input 3D image or label tensor. Expected shape: [1, H, W, D].\n",
    "        axis (int, optional): Axis along which to extract the slice (0, 1, or 2). Defaults to 2.\n",
    "        center (int, optional): Index of the slice to extract. If None, the middle slice is used.\n",
    "        mask_bool (bool, optional): If True, treat the input as a label mask and normalize it. Defaults to True.\n",
    "        n_label (int, optional): Number of labels in the mask. Used only if mask_bool is True. Defaults to 105.\n",
    "        colorize (torch.Tensor, optional): Colorization weights for label normalization.\n",
    "                                           Expected shape: [3, n_label, 1, 1] if provided.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: 2D slice of the input. If mask_bool is True, returns a normalized uint8 array\n",
    "                       with shape [3, H, W]. Otherwise, returns a float32 array with shape [3, H, W].\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the specified axis is not 0, 1, or 2.\n",
    "    \"\"\"\n",
    "    # draw image\n",
    "    if center is None:\n",
    "        center = image.shape[2:][axis] // 2\n",
    "    if axis == 0:\n",
    "        draw_img = image[..., center, :, :]\n",
    "    elif axis == 1:\n",
    "        draw_img = image[..., :, center, :]\n",
    "    elif axis == 2:\n",
    "        draw_img = image[..., :, :, center]\n",
    "    else:\n",
    "        raise ValueError(\"axis should be in [0,1,2]\")\n",
    "    if mask_bool:\n",
    "        draw_img = normalize_label_to_uint8(colorize, draw_img, n_label)\n",
    "    else:\n",
    "        draw_img = draw_img.squeeze().cpu().numpy().astype(np.float32)\n",
    "        draw_img = np.stack((draw_img,) * 3, axis=-1)\n",
    "    return draw_img\n",
    "\n",
    "\n",
    "def show_image(image, title=\"mask\"):\n",
    "    \"\"\"\n",
    "    Plot and display an input image.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): Image to be displayed. Expected shape: [H, W] for grayscale or [H, W, 3] for RGB.\n",
    "        title (str, optional): Title for the plot. Defaults to \"mask\".\n",
    "    \"\"\"\n",
    "    plt.figure(\"check\", (24, 12))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(title)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def to_shape(a, shape):\n",
    "    \"\"\"\n",
    "    Pad an image to a desired shape.\n",
    "\n",
    "    This function pads a 3D numpy array (image) with zeros to reach the specified shape.\n",
    "    The padding is added equally on both sides of each dimension, with any odd padding\n",
    "    added to the end.\n",
    "\n",
    "    Args:\n",
    "        a (numpy.ndarray): Input 3D array to be padded. Expected shape: [X, Y, Z].\n",
    "        shape (tuple): Desired output shape as (x_, y_, z_).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Padded array with the desired shape [x_, y_, z_].\n",
    "\n",
    "    Note:\n",
    "        If the input shape is larger than the desired shape in any dimension,\n",
    "        no padding is removed; the original size is maintained for that dimension.\n",
    "        Padding is done using numpy's pad function with 'constant' mode (zero-padding).\n",
    "    \"\"\"\n",
    "    x_, y_, z_ = shape\n",
    "    x, y, z = a.shape\n",
    "    x_pad = x_ - x\n",
    "    y_pad = y_ - y\n",
    "    z_pad = z_ - z\n",
    "    return np.pad(\n",
    "        a,\n",
    "        (\n",
    "            (x_pad // 2, x_pad // 2 + x_pad % 2),\n",
    "            (y_pad // 2, y_pad // 2 + y_pad % 2),\n",
    "            (z_pad // 2, z_pad // 2 + z_pad % 2),\n",
    "        ),\n",
    "        mode=\"constant\",\n",
    "    )\n",
    "\n",
    "\n",
    "def get_xyz_plot(image, center_loc_axis, mask_bool=True, n_label=105, colorize=None, target_class_index=0):\n",
    "    \"\"\"\n",
    "    Generate a concatenated XYZ plot of 2D slices from a 3D image.\n",
    "\n",
    "    This function creates visualizations of three orthogonal slices (XY, XZ, YZ) from a 3D image\n",
    "    and concatenates them into a single 2D image.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): Input 3D image tensor. Expected shape: [1, H, W, D].\n",
    "        center_loc_axis (list): List of three integers specifying the center locations for each axis.\n",
    "        mask_bool (bool, optional): Whether to apply masking. Defaults to True.\n",
    "        n_label (int, optional): Number of labels for visualization. Defaults to 105.\n",
    "        colorize (torch.Tensor, optional): Colorization weights. Expected shape: [3, n_label, 1, 1] if provided.\n",
    "        target_class_index (int, optional): Index of the target class. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Concatenated 2D image of the three orthogonal slices. Shape: [max(H,W,D), 3*max(H,W,D), 3].\n",
    "\n",
    "    Note:\n",
    "        The output image is padded to ensure all slices have the same dimensions.\n",
    "    \"\"\"\n",
    "    target_shape = list(image.shape[1:])  # [1,H,W,D]\n",
    "    img_list = []\n",
    "\n",
    "    for axis in range(3):\n",
    "        center = center_loc_axis[axis]\n",
    "\n",
    "        img = visualize_one_slice_in_3d(\n",
    "            torch.flip(image.unsqueeze(0), [-3, -2, -1]),\n",
    "            axis,\n",
    "            center=center,\n",
    "            mask_bool=mask_bool,\n",
    "            n_label=n_label,\n",
    "            colorize=colorize,\n",
    "        )\n",
    "        img = img.transpose([2, 1, 0])\n",
    "\n",
    "        img = to_shape(img, (3, max(target_shape), max(target_shape)))\n",
    "        img_list.append(img)\n",
    "        img = np.concatenate(img_list, axis=2).transpose([1, 2, 0])\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb5d6fa",
   "metadata": {},
   "source": [
    "\n",
    "## Execute inference\n",
    "\n",
    "* A pre-trained model for volumetric (3D) Brats MRI 3D Latent Diffusion Generative Model.\n",
    "\n",
    "* This model is trained on BraTS 2016 and 2017 data from [Medical Decathlon](http://medicaldecathlon.com/), using the Latent diffusion model [1].\n",
    "\n",
    "![model workflow](https://developer.download.nvidia.com/assets/Clara/Images/monai_brain_image_gen_ldm3d_network.png)\n",
    "\n",
    "* This model is a generator for creating images like the Flair MRIs based on BraTS 2016 and 2017 data. It was trained as a 3d latent diffusion model and accepts Gaussian random noise as inputs to produce an image output.\n",
    "\n",
    "* The following code generates a synthetic image from a random sampled noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e600fe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## method 1\n",
    "# !python -m monai.bundle run --config_file \"configs/inference.json\"\n",
    "\n",
    "## method 2\n",
    "# import monai\n",
    "# monai.bundle.run(config_file=\"configs/inference.json\")\n",
    "\n",
    "## method 3\n",
    "def generate_mri_sample(config_path: str, output_dir: str):\n",
    "    import glob\n",
    "    import os\n",
    "    # Capture existing output files\n",
    "    existing_files = set(glob.glob(os.path.join(output_dir, \"**\", \"*.nii.gz\"), recursive=True))\n",
    "\n",
    "    import monai\n",
    "    monai.bundle.run(config_file=config_path)\n",
    "\n",
    "    # Identify new files\n",
    "    all_files = set(glob.glob(os.path.join(output_dir, \"**\", \"*.nii.gz\"), recursive=True))\n",
    "    new_files = all_files - existing_files\n",
    "\n",
    "    if new_files:\n",
    "        return max(new_files, key=os.path.getmtime)  # Return the most recent MRI file\n",
    "    else:\n",
    "        return None  # No new MRI file detected\n",
    "\n",
    "filepath = generate_mri_sample(config_path=\"configs/inference.json\", output_dir=\"output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1978a03d",
   "metadata": {},
   "source": [
    "#### Example synthetic image\n",
    "An example result from inference is shown below:\n",
    "![Example synthetic image](https://developer.download.nvidia.com/assets/Clara/Images/monai_brain_image_gen_ldm3d_example_generation_v2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114bb8d7-17db-48b5-8d08-d8af61fc763a",
   "metadata": {},
   "source": [
    "## Visualize the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0453d9f-1614-4c84-aef1-77b6339d8c12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_image_filename = filepath #eg: \"./output/0/0_sample_20250129_060631.nii.gz\"\n",
    "print(f\"Visualizing {visualize_image_filename} ...\")\n",
    "\n",
    "# load image/mask pairs\n",
    "loader = LoadImage(image_only=True, ensure_channel_first=True)\n",
    "orientation = Orientation(axcodes=\"RAS\")\n",
    "image_volume = orientation(loader(visualize_image_filename))\n",
    "mask_volume = orientation(loader(visualize_image_filename)).to(torch.uint8)\n",
    "\n",
    "# visualize for CT HU intensity between [-200, 500]\n",
    "image_volume = torch.clip(image_volume, -200, 500)\n",
    "image_volume = image_volume - torch.min(image_volume)\n",
    "image_volume = image_volume / torch.max(image_volume)\n",
    "\n",
    "# create a random color map for mask visualization\n",
    "colorize = torch.clip(torch.cat([torch.zeros(3, 1, 1, 1), torch.randn(3, 200, 1, 1)], 1), 0, 1)\n",
    "target_class_index = 4\n",
    "\n",
    "# find center voxel location for 2D slice visualization\n",
    "center_loc_axis = find_label_center_loc(torch.flip(mask_volume[0, ...] == target_class_index, [-3, -2, -1]))\n",
    "\n",
    "\n",
    "vis_image = get_xyz_plot(image_volume, center_loc_axis, mask_bool=False)\n",
    "show_image(vis_image, title=\"image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c177dd",
   "metadata": {},
   "source": [
    "# interactive plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a10517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "img = image_volume[0]\n",
    "frames = [] # for storing the generated images\n",
    "fig = plt.figure()\n",
    "for i in range(img.shape[0]):\n",
    "    frames.append([plt.imshow(img[i], aspect=0.74, cmap=\"gray\", animated=True)])\n",
    "\n",
    "ani = animation.ArtistAnimation(fig, frames, interval=50, blit=True,\n",
    "                                repeat_delay=1000);                             \n",
    "# plt.show()\n",
    "# ani.save('mri.mp4')\n",
    "from IPython.display import HTML\n",
    "HTML(ani.to_jshtml())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3f8800",
   "metadata": {},
   "source": [
    "# MRI Image Segmentation\n",
    "\n",
    "* here we are segmenting the above generated MRI, using a pretrained segmentation model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77c1cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.networks.nets import SegResNet\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AsDiscrete,\n",
    "    Compose\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46468ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_AMP = True\n",
    "\n",
    "# standard PyTorch program style: create SegResNet, DiceLoss and Adam optimizer\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = SegResNet(\n",
    "    blocks_down=[1, 2, 2, 4],\n",
    "    blocks_up=[1, 1, 1],\n",
    "    init_filters=16,\n",
    "    in_channels=4,\n",
    "    out_channels=3,\n",
    "    dropout_prob=0.2,\n",
    ").to(device)\n",
    "\n",
    "post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "\n",
    "# define inference method\n",
    "def inference(input):\n",
    "    def _compute(input):\n",
    "        return sliding_window_inference(\n",
    "            inputs=input,\n",
    "            roi_size=(240, 240, 160),\n",
    "            sw_batch_size=1,\n",
    "            predictor=model,\n",
    "            overlap=0.5,\n",
    "        )\n",
    "\n",
    "    if VAL_AMP:\n",
    "        with torch.cuda.amp.autocast():\n",
    "            return _compute(input)\n",
    "    else:\n",
    "        return _compute(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ced1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(\"./models\", \"best_metric_model.pth\")))\n",
    "model.eval()\n",
    "\n",
    "layer = 10 #Participants can change this value to see the output at different layers, value ranges from 0 to 111.\n",
    "with torch.no_grad():\n",
    "    val_input = image_volume.unsqueeze(1).repeat(1, 4, 1, 1, 1).to(device)\n",
    "    val_output = inference(val_input)\n",
    "    print(val_input.shape, val_output.shape)\n",
    "    val_output = post_trans(val_output[0])\n",
    "    plt.figure(\"image\", (16, 12))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.title(f\"input image\")\n",
    "    plt.imshow(val_input[0][1, :, :, layer].detach().cpu(), cmap=\"gray\")\n",
    " \n",
    "    summed_output = torch.sum(val_output, dim=0, keepdim=True)\n",
    " \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.title(f\"output mask\")\n",
    "    plt.imshow(summed_output[0][:, :, layer].detach().cpu())\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.title(f\"mask on input\")\n",
    "    plt.imshow(val_input[0][1, :, :, layer].detach().cpu() + 0.3 * summed_output[0][:, :, layer].detach().cpu(), cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d3679f",
   "metadata": {},
   "source": [
    "# Suggested pipeline for continious training and integration\n",
    "\n",
    "* The following pipeline suggested a fully automated workflow to generate new synthetic MRI tumor segmenattion samples, which can be used for training new versions of this segmentation model.\n",
    "\n",
    "![pipeline diagram](./docs/monai-imagegen.drawio.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a10517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "py:percent,ipynb"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

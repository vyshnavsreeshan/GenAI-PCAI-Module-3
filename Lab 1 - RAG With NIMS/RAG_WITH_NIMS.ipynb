{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cec66cd",
   "metadata": {},
   "source": [
    "# RAG with NVIDIA NIM Microservices\n",
    "\n",
    "Welcome to this lab! In this notebook, you'll learn how to build a production-grade Retrieval-Augmented Generation (RAG) pipeline using NVIDIA NIM microservices.\n",
    "\n",
    "## What You'll Learn\n",
    "- **NVIDIA NIMs**: How to integrate hosted microservices for Embeddings, Reranking, and LLM generation.\n",
    "- **RAG Architecture**: Building a complete pipeline from ingestion to generation.\n",
    "- **Vector Stores**: Using FAISS for efficient similarity search.\n",
    "- **Guardrails**: Implementing topic control to keep the AI focused.\n",
    "\n",
    "## Technologies Used\n",
    "| Component | Technology | Purpose |\n",
    "|-----------|------------|---------|\n",
    "| **Embeddings** | `nvidia/nv-embed-v1` | High-performance text embeddings |\n",
    "| **LLM** | `meta/llama-3.2-1b-instruct` | Efficient instruction-tuned generation |\n",
    "| **Reranker** | `nvidia/llama-3.2-nv-rerankqa-1b-v2` | Improving retrieval relevance |\n",
    "| **Guardrails** | `llama-3.1-nemoguard-8b-topic-control` | Input/Output safety and steering |\n",
    "| **Orchestration** | LangChain | Pipeline management |\n",
    "| **Vector DB** | FAISS | Similarity search engine |\n",
    "\n",
    "## Lab Flow\n",
    "1.  **Setup**: Install dependencies and configure API keys.\n",
    "2.  **Initialization**: Connect to NVIDIA NIM clients.\n",
    "3.  **Ingestion**: Load PDFs, split text, and create vector embeddings.\n",
    "4.  **Basic RAG**: Retrieve documents and generate answers.\n",
    "5.  **Guardrails**: Apply topic control to restrict the assistant's scope.\n",
    "6.  **Advanced RAG**: Add reranking to improve answer quality.\n",
    "\n",
    "---\n",
    "\n",
    "## System Architecture\n",
    "\n",
    "The diagram below illustrates the RAG pipeline we will build.\n",
    "\n",
    "<img src=\"./docs/RAG_WITH_NIMS.png\" alt=\"RAG Architecture\" width=\"800\"/>\n",
    "\n",
    "### Architecture Description\n",
    "\n",
    "The system is designed in two main stages:\n",
    "\n",
    "#### 1. Data Ingestion (Preprocessing)\n",
    "- **Document Loader**: Reads PDF files from the `./pdf` directory.\n",
    "- **Text Splitter**: Breaks documents into manageable chunks (800 chars) with overlap to preserve context.\n",
    "- **Embedding Model (`nv-embed-v1`)**: Converts text chunks into dense vector representations.\n",
    "- **Vector Store (FAISS)**: Indexes these vectors for fast similarity search.\n",
    "\n",
    "#### 2. Inference Pipeline (User Query)\n",
    "- **Topic Control (`nemoguard-8b`)**: First, the user's query is checked against a specific prompt (e.g., \"Only answer HR questions\"). If off-topic, the system refuses politely.\n",
    "- **Retriever**: If on-topic, the system embeds the query and searches the FAISS index for the top 20 most similar chunks.\n",
    "- **Reranker (`nv-rerankqa-1b`)**: These 20 chunks are re-scored by a cross-encoder model to find the truly relevant ones, filtering out noise.\n",
    "- **LLM Generator (`llama-3.2-1b`)**: The top reranked documents are combined with the user's query into a prompt. The LLM generates a factual answer with citations.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be127c85",
   "metadata": {},
   "source": [
    "### Scenario: InnovateSphere Corporation\n",
    "\n",
    "We'll work with documents from InnovateSphere, a fictional company with three departments:\n",
    "- **HR**: Employee policies, leave entitlements, code of conduct\n",
    "- **Marketing**: Brand guidelines, campaigns, social media policies  \n",
    "- **Sales**: Product catalogs, sales reports, team directories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a82264e",
   "metadata": {},
   "source": [
    "### NVIDIA NIMs Used\n",
    "\n",
    "| Model | Purpose |\n",
    "|-------|----------|\n",
    "| `nvidia/nv-embed-v1` | Generate 4096-dim embeddings for semantic search |\n",
    "| `meta/llama-3.2-1b-instruct` | Generate natural language responses |\n",
    "| `nvidia/llama-3.2-nv-rerankqa-1b-v2` | Rerank documents by relevance |\n",
    "| `nvidia/llama-3.1-nemoguard-8b-topic-control` | Enforce topic boundaries (guardrails) |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86fcb6a",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "Install required packages for RAG implementation:\n",
    "- **LangChain ecosystem**: Framework for LLM applications\n",
    "- **FAISS**: Vector database for similarity search\n",
    "- **NVIDIA AI Endpoints**: Integration with NVIDIA NIMs\n",
    "- **PyPDF**: PDF document parsing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99e755f-47b3-4586-a8af-6852e8b95efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install compatible versions\n",
    "%pip install -q \\\n",
    "    pypdf \\\n",
    "    faiss-cpu \\\n",
    "    \"langchain>=0.3,<0.4\" \\\n",
    "    \"langchain-core>=0.3,<0.4\" \\\n",
    "    \"langchain-text-splitters>=0.3,<0.4\" \\\n",
    "    langchain-community \\\n",
    "    langchain_nvidia_ai_endpoints \\\n",
    "    openai \\\n",
    "    numpy==1.26.4\n",
    "\n",
    "print(\"Dependencies installed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0960477b",
   "metadata": {},
   "source": [
    "## 2. Configure NVIDIA API Key\n",
    "\n",
    "To authenticate with the NVIDIA API Catalog, you need to set your personal API key. This key allows you to access the hosted models via LangChain.\n",
    "\n",
    "If you haven't already generated your key, follow the step-by-step guide below.\n",
    "\n",
    "> **Important:** Never share your API key publicly or commit it to source control.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de72a68",
   "metadata": {},
   "source": [
    "### How to Generate Your NVIDIA API Key\n",
    "\n",
    "Follow these steps to generate your API key:\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 1: Log in to NVIDIA Build\n",
    "\n",
    "Go to the [NVIDIA API Keys page](https://build.nvidia.com/settings/api-keys) and log in using your NVIDIA account credentials.\n",
    "\n",
    "<img src=\"./docs/key_guide/login.png\" alt=\"Step 1: Log in to NVIDIA Build\" width=\"900\"/>\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2: Navigate to API Key Settings\n",
    "\n",
    "Once logged in, click on the **\"API Keys\"** tab in the sidebar or top navigation menu.\n",
    "\n",
    "<img src=\"./docs/key_guide/menu.png\" alt=\"Step 2: API Key Menu\" width=\"900\"/>\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3: Click \"Generate API Key\"\n",
    "\n",
    "Click the **\"Generate API Key\"** button to start creating a new key.\n",
    "\n",
    "<img src=\"./docs/key_guide/generate.png\" alt=\"Step 3: Generate API Key\" width=\"900\"/>\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 4: Fill Out the API Key Form\n",
    "\n",
    "You'll be prompted to fill in some details like a name and expiry time for the key. Complete the form and click **Generate key**.\n",
    "\n",
    "<img src=\"./docs/key_guide/form.png\" alt=\"Step 4: Fill API Key Form\" width=\"900\"/>\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 5: Copy and Store Your Key Securely\n",
    "\n",
    "After the key is generated, **copy it immediately** and store it somewhere safe. You **won't be able to view it again**.\n",
    "\n",
    "<img src=\"./docs/key_guide/copy.png\" alt=\"Step 5: Copy API Key\" width=\"900\"/>\n",
    "\n",
    "---\n",
    "\n",
    "### Set Your API Key\n",
    "\n",
    "Now that you have your API key, paste it in the code cell below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1cca21-4ce6-4769-831e-560465d97f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"API KEY HERE\" # Paste your actual API key here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046422c2",
   "metadata": {},
   "source": [
    "## 3. Initialize NVIDIA NIMs Clients\n",
    "\n",
    "Initialize four microservices:\n",
    "\n",
    "**Embeddings (`nv-embed-v1`)**: Converts text to dense vectors for semantic search. Based on Mistral-7B with Latent-Attention pooling.\n",
    "\n",
    "**LLM (`llama-3.2-1b-instruct`)**: 1B parameter instruction-tuned model for answer generation. Configured with low temperature (0.2) for factual responses.\n",
    "\n",
    "**Reranker (`llama-3.2-nv-rerankqa-1b-v2`)**: Re-scores retrieved documents for improved relevance. Supports up to 8192 tokens.\n",
    "\n",
    "**Topic Control (`llama-3.1-nemoguard-8b-topic-control`)**: Classifies queries as on-topic/off-topic to enforce conversational boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947dfd5b-4fc5-4d74-b010-fabe1403177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings, NVIDIARerank\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# Initialize the Embeddings client to get vector representations of documents and queries\n",
    "embedding_client = NVIDIAEmbeddings(\n",
    "    model=\"nvidia/nv-embed-v1\",\n",
    "    api_key=API_KEY,\n",
    "    truncate=\"NONE\",\n",
    ")\n",
    "\n",
    "# Initialize Chat client for LLM generation\n",
    "gpt_client = ChatNVIDIA(\n",
    "    model=\"meta/llama-3.2-1b-instruct\",\n",
    "    api_key=API_KEY,\n",
    "    temperature=0.2,\n",
    "    top_p=0.5,\n",
    ")\n",
    "\n",
    "# Initialize the reranker client for passage reranking\n",
    "reranker_client = NVIDIARerank(\n",
    "    model=\"nvidia/llama-3.2-nv-rerankqa-1b-v2\",\n",
    "    api_key=API_KEY,\n",
    ")\n",
    "\n",
    "# Initialize Topic Control client for guardrails\n",
    "topic_control_client = OpenAI(\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "    api_key=API_KEY,\n",
    ")\n",
    "\n",
    "print(\"NVIDIA clients initialized successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7ddb52",
   "metadata": {},
   "source": [
    "## 4. Load PDF Documents\n",
    "\n",
    "Load all PDFs from the `./pdf` directory recursively. The loader extracts text and preserves metadata (source file, page numbers).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab18d354-4a09-4f47-8e99-fdf39717cda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "loader = PyPDFDirectoryLoader(\"./pdf\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8264265",
   "metadata": {},
   "source": [
    "## 5. Chunk Documents\n",
    "\n",
    "Split documents into 800-character chunks with 60-character overlap. This balances:\n",
    "- **Chunk size**: Small enough for focused retrieval, large enough for context\n",
    "- **Overlap**: Prevents information loss at chunk boundaries\n",
    "\n",
    "The splitter intelligently preserves semantic coherence by splitting on paragraphs, then sentences, then words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfe2af4-c711-4507-a29b-9fbe8b2d2823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=60,\n",
    ")\n",
    "documents = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5356bbd4",
   "metadata": {},
   "source": [
    "## 6. Create Vector Store\n",
    "\n",
    "Generate embeddings for all chunks and build a FAISS index for fast similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37a93db-4fda-4890-abd6-64699e3aa3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain.vectorstores import FAISS\n",
    "docsearch = FAISS.from_documents(documents, embedding=embedding_client)\n",
    "docsearch.save_local(folder_path=\"./embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a23a5c",
   "metadata": {},
   "source": [
    "## 7. Load Pre-computed Embeddings\n",
    "\n",
    "Load the existing FAISS index from disk. This contains embeddings for all document chunks.\n",
    "\n",
    "> **Security**: `allow_dangerous_deserialization=True` is required for pickle-based serialization. Only load from trusted sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4dbb44-4aaa-4d14-91ab-3f65ca1158c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "docsearch= FAISS.load_local(\n",
    "    \"./embeddings\" , embedding_client, allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622a3d5d",
   "metadata": {},
   "source": [
    "## 8. Configure Retriever\n",
    "\n",
    "Create a retriever that returns the top 20 most semantically similar chunks for each query. The retriever:\n",
    "1. Embeds the query using `nv-embed-v1`\n",
    "2. Performs cosine similarity search in FAISS\n",
    "3. Returns the k=20 nearest document chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6127f194-ca9b-4a04-bc80-f8c9825ff29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever(search_kwargs={\"k\": 20}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3931b5d4",
   "metadata": {},
   "source": [
    "## 9. Define Topic Control Prompts (Guardrails)\n",
    "\n",
    "Define domain-specific prompts that restrict AI assistants to their designated topics:\n",
    "\n",
    "- **HR Assistant**: Only answers HR-related questions (policies, leave, conduct)\n",
    "- **Marketing Assistant**: Only answers marketing questions (campaigns, branding)\n",
    "- **Sales Assistant**: Only answers sales questions (products, reports)\n",
    "\n",
    "These guardrails prevent assistants from answering out-of-scope questions, ensuring information security and role-based access control.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69744321-a3a0-4a40-beeb-05cf29fff0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "HR_TOPIC_CONTROL_PROMPT = (\n",
    "    \"You are an HR assistant for InnovateSphere. Only answer questions about HR policies, employee handbook, leave entitlements, Code of Conduct, diversity, workplace guidelines, or performance review procedures. Do not answer any queries about marketing, sales, products, financials, or campaigns. If the query is outside HR topics, respond that you cannot provide such information.\"\n",
    ")\n",
    "MARKETING_TOPIC_CONTROL_PROMPT = (\n",
    "    \"You are a marketing assistant for InnovateSphere. Only answer questions about InnovateSphere’s marketing campaigns, brand guidelines, messaging, target markets, and social media policies. Do not address HR policies, sales results, product information, or internal staff matters. Respond with a polite rejection if the question is outside marketing topics.\"\n",
    ")\n",
    "SALES_TOPIC_CONTROL_PROMPT = (\n",
    "    \"You are a sales assistant for InnovateSphere. Provide information only about sales reports, product catalog, sales team directory, and quarterly performance. Never answer questions relating to HR, marketing, or internal policies. If the query is not about sales, reject it as outside sales scope.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a140028",
   "metadata": {},
   "source": [
    "## 10. Import RAG Pipeline\n",
    "\n",
    "Import the custom `RAGPipeline` class that orchestrates:\n",
    "- Topic control (optional guardrails)\n",
    "- Document retrieval\n",
    "- Reranking (optional quality improvement)\n",
    "- LLM answer generation\n",
    "- Source citation and formatting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a364a53-6fd8-4820-9b61-c3977fe7f22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import RAGPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d76d63",
   "metadata": {},
   "source": [
    "## 11. Initialize RAG Pipeline\n",
    "\n",
    "Create a pipeline instance with all NVIDIA NIMs clients. The pipeline provides a unified interface:\n",
    "\n",
    "```python\n",
    "pipeline.query(\n",
    "    query=\"Your question\",\n",
    "    enable_topic_control=True,  # Optional: Enable guardrails\n",
    "    enable_rerank=True          # Optional: Enable reranking\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0cd069-a2fb-4d8d-8151-5985a5b52a20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_control_model = \"nvidia/llama-3.1-nemoguard-8b-topic-control\"\n",
    "\n",
    "pipeline = RAGPipeline.RAGPipeline(\n",
    "    retriever=retriever,\n",
    "    gpt_client=gpt_client,\n",
    "    reranker_client=reranker_client,\n",
    "    topic_control_client=topic_control_client,\n",
    "    topic_control_model=topic_control_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9966c58",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Basic RAG Queries\n",
    "\n",
    "Test the RAG system without guardrails or reranking. The pipeline will:\n",
    "1. Retrieve top 20 relevant chunks\n",
    "2. Generate an answer using the LLM\n",
    "3. Display source documents with citations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b36c544",
   "metadata": {},
   "source": [
    "### Query 1: HR Content\n",
    "\n",
    "Ask about InnovateSphere's professional integrity statement from HR documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047fa304-1113-43aa-a33f-36fd3fa8cb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.query(\"What is the professional integrity statement of InnovateSphere?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06550184",
   "metadata": {},
   "source": [
    "### Query 2: Sales Content\n",
    "\n",
    "Retrieve Q1 sales performance figures from sales reports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fcbad1-f16f-4960-a224-89ec911ca1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.query(\"Show the Q1 sales performance figures.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5af91cb",
   "metadata": {},
   "source": [
    "### Query 3: Marketing Content\n",
    "\n",
    "Query brand identity guidelines from marketing documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baa55e3-8338-4504-a3e2-f4a796a73074",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.query(\"Describe InnovateSphere's brand identity guidelines.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523db369",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: RAG with Topic Control (Guardrails)\n",
    "\n",
    "Demonstrate how guardrails enforce domain boundaries. The topic control model classifies queries as \"on-topic\" or \"off-topic\" based on the configured prompt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ded50ed",
   "metadata": {},
   "source": [
    "### Demo 1: HR Assistant - On-Topic Query\n",
    "\n",
    "Configure as HR assistant and ask an HR-related question. Expected: Answer provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d79c950-a1c8-482e-993b-30f34e0c7b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.topic_control_prompt = HR_TOPIC_CONTROL_PROMPT\n",
    "pipeline.query(\"How many days of annual leave do employees receive?\", enable_topic_control=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8882a9c4",
   "metadata": {},
   "source": [
    "### Demo 2: HR Assistant - Off-Topic Query\n",
    "\n",
    "Ask the HR assistant a sales question. Expected: Polite refusal explaining the question is out of scope.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b49a2a-77e9-4551-aaac-0ed8089200f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline.topic_control_prompt = HR_TOPIC_CONTROL_PROMPT\n",
    "pipeline.query(\"Provide the product catalog details.\", enable_topic_control=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131487c6",
   "metadata": {},
   "source": [
    "### Demo 3: Sales Assistant - Same Query\n",
    "\n",
    "Ask the same product catalog question to a Sales assistant. Expected: Answer provided (now on-topic).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53520d76-6b53-42dc-8e39-d9d62bb40a51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline.topic_control_prompt = SALES_TOPIC_CONTROL_PROMPT\n",
    "pipeline.query(\"Provide the product catalog details.\", enable_topic_control=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6d24bd",
   "metadata": {},
   "source": [
    "### Demo 4: Marketing Assistant\n",
    "\n",
    "Test marketing domain guardrails with a campaign-related query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dc4096-3335-4fef-b41b-16ffa19ffe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.topic_control_prompt = MARKETING_TOPIC_CONTROL_PROMPT\n",
    "pipeline.query(\"What is the target persona for the SynergyHub campaign?\", enable_topic_control=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45257436",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: RAG with Reranking\n",
    "\n",
    "Enable reranking to improve retrieval quality. The reranker:\n",
    "1. Takes the initial 20 retrieved documents\n",
    "2. Compares each document to the query using a cross-encoder\n",
    "3. Re-scores and reorders documents by relevance\n",
    "4. Provides relevance scores in the output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1d2287",
   "metadata": {},
   "source": [
    "### Query with Reranking\n",
    "\n",
    "Ask about performance review criteria with reranking enabled. Notice the relevance scores in the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f0cf31-7fb8-42df-a7bb-614c84a32356",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.query(\"Detail performance review criteria for staff.\", enable_rerank=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87d4226",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Full Pipeline (Topic Control + Reranking)\n",
    "\n",
    "Combine both features for production-grade RAG:\n",
    "- **Topic Control**: Ensures queries are within scope\n",
    "- **Reranking**: Maximizes answer quality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4806891d",
   "metadata": {},
   "source": [
    "### Full Pipeline Demo 1: HR Domain\n",
    "\n",
    "HR assistant with both guardrails and reranking enabled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddefe371-4a70-49f1-8548-3b48eb6be9da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline.topic_control_prompt = HR_TOPIC_CONTROL_PROMPT\n",
    "pipeline.query(\"Explain the diversity commitment at InnovateSphere.\", enable_topic_control=True, enable_rerank=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bc28c5",
   "metadata": {},
   "source": [
    "### Full Pipeline Demo 2: Marketing Domain\n",
    "\n",
    "Marketing assistant with full features enabled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834f33f7-a44a-4973-a95b-d709cba4cf42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline.topic_control_prompt = MARKETING_TOPIC_CONTROL_PROMPT\n",
    "pipeline.query(\"What are the brand usage rules?\", enable_topic_control=True, enable_rerank=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddedb442",
   "metadata": {},
   "source": [
    "### Full Pipeline Demo 3: Sales Domain\n",
    "\n",
    "Sales assistant with topic control and reranking for optimal performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7ef0e3-98da-4cdd-b2f2-d83293a3f393",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.topic_control_prompt = SALES_TOPIC_CONTROL_PROMPT\n",
    "pipeline.query(\"Provide the product catalog details\", enable_topic_control=True, enable_rerank=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "You've successfully built a production-grade RAG system with:\n",
    "\n",
    "✅ **Semantic Search**: FAISS vector database with NVIDIA embeddings  \n",
    "✅ **LLM Generation**: Context-aware answers with citations  \n",
    "✅ **Guardrails**: Topic control for domain-specific assistants  \n",
    "✅ **Quality Optimization**: Reranking for improved relevance  \n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Experiment with your own queries\n",
    "- Compare results with/without reranking\n",
    "- Create custom topic control prompts\n",
    "- Adjust retrieval parameters (k, temperature, top_p)\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [NVIDIA API Catalog](https://build.nvidia.com/)\n",
    "- [NeMo Guardrails](https://github.com/NVIDIA/NeMo-Guardrails)\n",
    "- [LangChain Docs](https://python.langchain.com/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
